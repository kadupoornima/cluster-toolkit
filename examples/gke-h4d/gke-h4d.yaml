# Copyright 2025 "Google LLC"
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

blueprint_name: gke-h4d

vars:
  # The following variables should be over-written in the deployment.yaml file.
  # Your GCP Project ID
  project_id:

  # This should be unique across all of your Cluster
  # Toolkit Deployments.
  deployment_name: gke-h4d

  # The GCP Region used for this deployment.
  region:

  # The GCP Zone used for this deployment.
  zone:

  # The number of nodes to be created.
  static_node_count:

  # Cidr block containing the IP of the machine calling terraform.
  # The following line must be updated for this example to work.
  authorized_cidr:

  # The name of the compute engine reservation in the form of
  # <reservation-name>
  # To target a BLOCK_NAME, the name of the extended reservation
  # can be inputted as <reservation-name>/reservationBlocks/<reservation-block-name>
  reservation:

  system_node_pool_disk_size_gb: 100
  h4d_node_pool_disk_size_gb: 100


deployment_groups:
- group: primary
  modules:
  - id: gke-h4d-net
    source: modules/network/vpc
    settings:
      network_name: $(vars.deployment_name)-net
      subnetworks:
      - subnet_name: $(vars.deployment_name)-sub
        subnet_region: $(vars.region)
        subnet_ip: 192.168.0.0/24
      secondary_ranges_list:
      - subnetwork_name: $(vars.deployment_name)-sub
        ranges:
        - range_name: pods
          ip_cidr_range: 10.64.0.0/19
        - range_name: services
          ip_cidr_range: 10.65.0.0/19
      firewall_rules:
      - name: $(vars.deployment_name)-internal
        ranges: [192.168.0.0/24]
        allow:
        - protocol: tcp
          ports: ["0-65535"]
        - protocol: udp
          ports: ["0-65535"]
        - protocol: icmp

  - id: gke-h4d-rdma-net
    source: modules/network/vpc
    settings:
      network_name: $(vars.deployment_name)-rdma-net
      network_profile: https://www.googleapis.com/compute/beta/projects/$(vars.project_id)/global/networkProfiles/$(vars.zone)-vpc-falcon
      network_routing_mode: REGIONAL
      enable_cloud_router: false
      enable_cloud_nat: false
      subnetworks:
      - subnet_name: $(vars.deployment_name)-rdma-sub
        subnet_region: $(vars.region)
        subnet_ip: 192.168.1.0/24
        region: $(vars.region)

  - id: node_pool_service_account
    source: community/modules/project/service-account
    settings:
      name: gke-np-sa
      project_roles:
      - logging.logWriter
      - monitoring.metricWriter
      - monitoring.viewer
      - stackdriver.resourceMetadata.writer
      - storage.objectViewer
      - artifactregistry.reader

  - id: workload_service_account
    source: community/modules/project/service-account
    settings:
      name: gke-wl-sa
      project_roles:
      - logging.logWriter
      - monitoring.metricWriter
      - monitoring.viewer
      - stackdriver.resourceMetadata.writer
      - storage.objectAdmin
      - artifactregistry.reader

  - id: h4d-cluster
    source: modules/scheduler/gke-cluster
    use: [gke-h4d-net, workload_service_account]
    settings:
      system_node_pool_machine_type: "e2-standard-16"
      system_node_pool_disk_size_gb: $(vars.system_node_pool_disk_size_gb)
      system_node_pool_taints: []
      enable_multi_networking: true
      enable_dcgm_monitoring: true
      enable_private_endpoint: false # Allows access from authorized public IPs
      configure_workload_identity_sa: true
      master_authorized_networks:
      - cidr_block: $(vars.authorized_cidr) # Allows your machine to run the kubectl command. Required for multi network setup.
        display_name: "kubectl-access-network"
      additional_networks:
        $(concat(
          [{
            network=gke-h4d-rdma-net.network_name,
            subnetwork=gke-h4d-rdma-net.subnetwork_name,
            subnetwork_project=vars.project_id,
            nic_type="IRDMA",
            queue_count=null,
            network_ip=null,
            stack_type=null,
            access_config=[{nat_ip=null, public_ptr_domain_name=null, network_tier=null}],
            ipv6_access_config=[],
            alias_ip_range=[]
          }]
        ))
      # Cluster versions cannot be updated through the toolkit after creation
      # Please manage cluster version from the Google Cloud Console directly
      version_prefix: "1.32."
      release_channel: RAPID
      maintenance_exclusions:
      - name: no-minor-or-node-upgrades-indefinite
        start_time: "2024-12-01T00:00:00Z"
        end_time: "2025-12-22T00:00:00Z"
        exclusion_scope: NO_MINOR_OR_NODE_UPGRADES
    outputs: [instructions]

  - id: h4d-pool
    source: modules/compute/gke-node-pool
    use: [h4d-cluster, node_pool_service_account]
    settings:
      machine_type: h4d-highmem-192-lssd
      auto_upgrade: true
      zones: [$(vars.zone)]
      disk_size_gb: $(vars.h4d_node_pool_disk_size_gb)
      static_node_count: $(vars.static_node_count)
      reservation_affinity:
        consume_reservation_type: SPECIFIC_RESERVATION
        specific_reservations:
        - name: $(vars.reservation)
      taints:
      # (Optional, but suggested)
      # In order prevent scheduling of generic workloads onto the H4D nodepool,
      # we suggest a Kubernetes taint. In order to schedule workloads, add the following
      # tolerance to a PodSpec that you want to allow to use the H4D nodes. See
      # https://kubernetes.io/docs/concepts/scheduling-eviction/taint-and-toleration/ for
      # details
      # tolerations:
      # - key: "node-type"
      #   operator: "Equal"
      #   value: "h4d"
      #   effect: "NoSchedule"
      - key: node-type
        value: h4d
        effect: NO_SCHEDULE
      placement_policy:
        type: COMPACT
      additional_networks:
        $(concat(
          [{
            network=gke-h4d-rdma-net.network_name,
            subnetwork=gke-h4d-rdma-net.subnetwork_name,
            subnetwork_project=vars.project_id,
            nic_type="IRDMA",
            queue_count=null,
            network_ip=null,
            stack_type=null,
            access_config=[{nat_ip=null, public_ptr_domain_name=null, network_tier=null}],
            ipv6_access_config=[],
            alias_ip_range=[]
          }]
        ))
    outputs: [instructions]

  # Install Kueue and Jobset
  - id: workload-manager-install
    source: modules/management/kubectl-apply
    use: [h4d-cluster]
    settings:
      kueue:
        install: true
      jobset:
        install: true
      apply_manifests:
      - source: "https://raw.githubusercontent.com/kubeflow/mpi-operator/v0.6.0/deploy/v2beta1/mpi-operator.yaml"
        server_side_apply: true

  # Filestore
  - id: filestore
    source: modules/file-system/filestore
    use: [gke-h4d-net]
    settings: {local_mount: /mnt/h4d-filestore}

  - id: shared-filestore-pv
    source: modules/file-system/gke-persistent-volume
    use: [h4d-cluster, filestore]

  # Shared Filestore Job
  - id: shared-fs-job
    source: modules/compute/gke-job-template
    use:
    - h4d-cluster
    - h4d-pool
    - shared-filestore-pv
    settings:
      image: alpine/git:latest
      command:
      - sh
      - -c
      - |
        echo "Pod ${HOSTNAME} is starting..."
        SHARED_DIR=/mnt/h4d-filestore
        mkdir -p $SHARED_DIR
        cd $SHARED_DIR

        # Simulate some work and write to a shared file
        cmd="date +%s"
        TIMESTAMP=`$cmd`
        echo "Pod ${HOSTNAME} writing data at $$TIMESTAMP" >> shared_output.txt
        echo "Displaying content of shared_output.txt:"
        echo "---"
        cat shared_output.txt # Read the content to show it's shared
        echo "---"
        sleep 120
        echo "Pod ${HOSTNAME} finished."
      node_count: 2
    outputs: [instructions]
